{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f55474",
   "metadata": {},
   "source": [
    "\n",
    "# Predictive Modelling of Eating-Out (Sydney) â€” Endâ€‘toâ€‘End Workflow\n",
    "\n",
    "This notebook implements the full pipeline requested in the assignment:\n",
    "- **Part A:** EDA (including geospatial)\n",
    "- **Part B:** Predictive modelling (regression & classification, incl. a custom gradientâ€‘descent linear regression)\n",
    "- **Part C:** Reproducibility scaffolding (Git + Git LFS + DVC)\n",
    "- **PySpark** equivalents for one regression and one classification task\n",
    "\n",
    "> **Data files expected** (put them next to this notebook unless you change the paths below):\n",
    "> - `zomato_df_final_data.csv`\n",
    "> - `sydney.geojson`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e978037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Config: paths & options ===\n",
    "DATA_CSV_PATH = \"zomato_df_final_data.csv\"     # change if stored elsewhere\n",
    "SYDNEY_GEOJSON_PATH = \"sydney.geojson\"         # change if stored elsewhere\n",
    "\n",
    "# Reproducibility\n",
    "import os, random, numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# Optional: widen pandas display for comfort\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b844b1",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Environment & dependencies\n",
    "\n",
    "Uncomment and run the next cell if any import fails. (You can also install these in a virtualenv/conda env and run the notebook there.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9765e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you need to install, uncomment:\n",
    "# %pip install -q pandas numpy scikit-learn matplotlib plotly geopandas shapely pyproj folium\n",
    "# %pip install -q pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2061b",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load & inspect dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd666d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(DATA_CSV_PATH, low_memory=False)\n",
    "print(\"Rows, Cols:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.info()\n",
    "print(\"\\nMissing values per column:\")\n",
    "df.isna().sum().sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Harmonize expected column names safely (some datasets use slightly different names)\n",
    "def pick_first_present(possible_names):\n",
    "    for name in possible_names:\n",
    "        if name in df.columns:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "COL_CUISINE = pick_first_present([\"cuisine\", \"cuisines\"])\n",
    "COL_RATING_NUM = pick_first_present([\"rating_number\", \"aggregate_rating\", \"rating\"])\n",
    "COL_RATING_TEXT = pick_first_present([\"rating_text\", \"rating_category\", \"rating_text_label\"])\n",
    "COL_SUBURB = pick_first_present([\"subzone\", \"suburb\", \"Suburb\", \"zone\", \"locality\"])\n",
    "COL_COST = pick_first_present([\"cost\", \"average_cost_for_two\", \"cost_for_two\"])\n",
    "COL_VOTES = pick_first_present([\"votes\", \"rating_votes\", \"num_votes\"])\n",
    "COL_TYPE = pick_first_present([\"type\", \"rest_type\", \"restaurant_type\"])\n",
    "COL_LAT = pick_first_present([\"lat\", \"latitude\"])\n",
    "COL_LNG = pick_first_present([\"lng\", \"longitude\", \"lon\"])\n",
    "\n",
    "print(\"Detected columns:\",\n",
    "      dict(cuisine=COL_CUISINE, rating_number=COL_RATING_NUM, rating_text=COL_RATING_TEXT,\n",
    "           suburb=COL_SUBURB, cost=COL_COST, votes=COL_VOTES, type=COL_TYPE,\n",
    "           lat=COL_LAT, lng=COL_LNG))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a20ff7",
   "metadata": {},
   "source": [
    "\n",
    "### Basic cleaning\n",
    "- Coerce numeric fields\n",
    "- Trim strings\n",
    "- Make cuisine a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf067ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "if COL_COST:\n",
    "    df[COL_COST] = pd.to_numeric(df[COL_COST], errors=\"coerce\")\n",
    "if COL_VOTES:\n",
    "    df[COL_VOTES] = pd.to_numeric(df[COL_VOTES], errors=\"coerce\")\n",
    "if COL_RATING_NUM:\n",
    "    df[COL_RATING_NUM] = pd.to_numeric(df[COL_RATING_NUM], errors=\"coerce\")\n",
    "\n",
    "for c in [COL_CUISINE, COL_RATING_TEXT, COL_SUBURB, COL_TYPE]:\n",
    "    if c and c in df.columns and df[c].dtype == object:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# cuisine into list of tokens (split on comma)\n",
    "if COL_CUISINE:\n",
    "    df[\"_cuisine_list\"] = df[COL_CUISINE].fillna(\"\").apply(lambda s: [x.strip() for x in str(s).split(\",\") if x.strip()])\n",
    "else:\n",
    "    df[\"_cuisine_list\"] = [[] for _ in range(len(df))]\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f7c11",
   "metadata": {},
   "source": [
    "\n",
    "## 2) EDA\n",
    "\n",
    "### 2.1 Unique cuisines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "c_counter = Counter()\n",
    "for items in df[\"_cuisine_list\"]:\n",
    "    c_counter.update(items)\n",
    "\n",
    "cuisine_counts = pd.Series(c_counter).sort_values(ascending=False)\n",
    "print(\"Unique cuisines:\", cuisine_counts.shape[0])\n",
    "cuisine_counts.head(20).to_frame(\"count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcf29a",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Top 3 suburbs with the most restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5345045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if COL_SUBURB:\n",
    "    top_suburbs = df[COL_SUBURB].value_counts().head(3)\n",
    "    top_suburbs.to_frame(\"restaurant_count\")\n",
    "else:\n",
    "    print(\"No suburb-like column detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7036d6b",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Are \"Excellent\" restaurants more expensive than \"Poor\" ones?\n",
    "We compare the distribution of costs for two rating groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if COL_RATING_TEXT and COL_COST:\n",
    "    groups = {\n",
    "        \"Class1_PoorAvg\": [\"Poor\", \"Average\"],\n",
    "        \"Class2_GoodUp\": [\"Good\", \"Very Good\", \"Excellent\"]\n",
    "    }\n",
    "    df[\"_rating_bin\"] = np.nan\n",
    "    df.loc[df[COL_RATING_TEXT].isin(groups[\"Class1_PoorAvg\"]), \"_rating_bin\"] = \"Poor+Average\"\n",
    "    df.loc[df[COL_RATING_TEXT].isin(groups[\"Class2_GoodUp\"]), \"_rating_bin\"] = \"Good/VeryGood/Excellent\"\n",
    "\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    df.boxplot(column=COL_COST, by=\"_rating_bin\")\n",
    "    plt.title(f\"Cost by rating group\"); plt.suptitle(\"\")\n",
    "    plt.xlabel(\"Rating group\"); plt.ylabel(\"Cost (for two)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Missing COL_RATING_TEXT or COL_COST -> skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7524f",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 Distributions and correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if COL_COST:\n",
    "    df[COL_COST].hist(bins=40, figsize=(6,4)); plt.title(\"Cost distribution\"); plt.show()\n",
    "if COL_RATING_NUM:\n",
    "    df[COL_RATING_NUM].hist(bins=40, figsize=(6,4)); plt.title(\"Rating (numeric) distribution\"); plt.show()\n",
    "if COL_VOTES:\n",
    "    df[COL_VOTES].hist(bins=40, figsize=(6,4)); plt.title(\"Votes distribution\"); plt.show()\n",
    "\n",
    "if COL_COST and COL_VOTES:\n",
    "    ax = df.plot(kind=\"scatter\", x=COL_COST, y=COL_VOTES, alpha=0.3, figsize=(6,4), title=\"Cost vs Votes\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1625c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Geospatial cuisine density (Sydney suburbs)\n",
    "\n",
    "We aggregate counts per suburb for a chosen cuisine and plot a choropleth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, folium\n",
    "import geopandas as gpd\n",
    "\n",
    "def guess_suburb_key(geo):\n",
    "    # Try a few common keys on the GeoJSON properties\n",
    "    candidates = [\"suburb\", \"Suburb\", \"name\", \"NAME\", \"nsw_loca_2\", \"nsw_loca_2_name\"]\n",
    "    sample_props = None\n",
    "    for feat in geo[\"features\"][:5]:\n",
    "        sample_props = feat.get(\"properties\", {})\n",
    "        break\n",
    "    for k in candidates:\n",
    "        if sample_props and k in sample_props:\n",
    "            return k\n",
    "    # fallback: first string-looking property\n",
    "    if sample_props:\n",
    "        for k,v in sample_props.items():\n",
    "            if isinstance(v, str):\n",
    "                return k\n",
    "    return None\n",
    "\n",
    "# load geojson\n",
    "with open(SYDNEY_GEOJSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    geo = json.load(f)\n",
    "\n",
    "SUBURB_KEY_GEOJSON = guess_suburb_key(geo)\n",
    "print(\"GeoJSON suburb key:\", SUBURB_KEY_GEOJSON)\n",
    "\n",
    "# helper to compute counts per suburb for a cuisine term\n",
    "def cuisine_density_for(term: str):\n",
    "    term_low = term.strip().lower()\n",
    "    if not COL_SUBURB:\n",
    "        raise ValueError(\"No suburb column found in dataset.\")\n",
    "    mask = df[\"_cuisine_list\"].apply(lambda L: any(term_low == x.lower() for x in L))\n",
    "    agg = df.loc[mask].groupby(COL_SUBURB).size().rename(\"count\").reset_index()\n",
    "    return agg\n",
    "\n",
    "# Build choropleth for a cuisine term\n",
    "def plot_cuisine_choropleth(term=\"Chinese\"):\n",
    "    agg = cuisine_density_for(term)\n",
    "    m = folium.Map(location=[-33.8688, 151.2093], zoom_start=10, control_scale=True)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame.from_features(geo[\"features\"])\n",
    "    # rename to common name for merge\n",
    "    if SUBURB_KEY_GEOJSON not in gdf.columns:\n",
    "        raise ValueError(f\"Could not find suburb key '{SUBURB_KEY_GEOJSON}' in GeoJSON columns: {gdf.columns.tolist()}\")\n",
    "    gdf = gdf.rename(columns={SUBURB_KEY_GEOJSON: \"suburb_key\"})\n",
    "    agg2 = agg.rename(columns={COL_SUBURB: \"suburb_key\"})\n",
    "\n",
    "    merged = gdf.merge(agg2, on=\"suburb_key\", how=\"left\").fillna({\"count\": 0})\n",
    "\n",
    "    folium.Choropleth(\n",
    "        geo_data=json.loads(merged.to_json()),\n",
    "        data=merged,\n",
    "        columns=[\"suburb_key\", \"count\"],\n",
    "        key_on=\"feature.properties.suburb_key\",\n",
    "        fill_color=\"YlOrRd\",\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.2,\n",
    "        nan_fill_opacity=0.1,\n",
    "        legend_name=f\"{term} restaurants per suburb\",\n",
    "    ).add_to(m)\n",
    "\n",
    "    folium.GeoJson(\n",
    "        data=json.loads(merged.to_json()),\n",
    "        name=\"Suburbs\",\n",
    "        tooltip=folium.GeoJsonTooltip(fields=[\"suburb_key\", \"count\"], aliases=[\"Suburb\", \"Count\"])\n",
    "    ).add_to(m)\n",
    "\n",
    "    return m\n",
    "\n",
    "# Example (change the cuisine term as needed):\n",
    "# m = plot_cuisine_choropleth(\"Chinese\"); m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b1f06",
   "metadata": {},
   "source": [
    "\n",
    "## 4) One interactive visual (Plotly)\n",
    "\n",
    "We'll re-create a scatter that benefits from hover and zoom: **Cost vs Rating**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "\n",
    "if COL_COST and COL_RATING_NUM:\n",
    "    fig = px.scatter(df, x=COL_COST, y=COL_RATING_NUM, trendline=\"ols\",\n",
    "                     title=\"Interactive: Cost vs Rating (hover & zoom)\",\n",
    "                     labels={COL_COST: \"Cost (for two)\", COL_RATING_NUM: \"Rating (numeric)\"})\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Missing numeric rating or cost column for this interactive demo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dab405",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Feature engineering for models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Derive helpful features\n",
    "df[\"cuisine_diversity\"] = df[\"_cuisine_list\"].apply(len)\n",
    "\n",
    "# Binary target for classification based on rating_text\n",
    "def map_rating_to_binary(s: str):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s).strip()\n",
    "    if s in [\"Poor\", \"Average\"]:\n",
    "        return 0\n",
    "    if s in [\"Good\", \"Very Good\", \"Excellent\"]:\n",
    "        return 1\n",
    "    return np.nan  # drop 'Not rated' or others\n",
    "\n",
    "y_cls = df[COL_RATING_TEXT].apply(map_rating_to_binary) if COL_RATING_TEXT else None\n",
    "\n",
    "# Columns for features\n",
    "num_cols = [c for c in [COL_COST, COL_VOTES, COL_RATING_NUM] if c]\n",
    "cat_cols = [c for c in [COL_TYPE, COL_SUBURB] if c]\n",
    "\n",
    "# We will also add cuisine one-hot (top K cuisines as binary flags)\n",
    "TOP_K = 20\n",
    "top_k_cuisines = [c for c,_ in Counter([x for L in df[\"_cuisine_list\"] for x in L]).most_common(TOP_K)]\n",
    "\n",
    "for c in top_k_cuisines:\n",
    "    df[f\"cuisine__{c}\"] = df[\"_cuisine_list\"].apply(lambda L, cc=c: int(cc in L))\n",
    "\n",
    "num_cols_extended = num_cols + [\"cuisine_diversity\"]\n",
    "cat_cols_extended = cat_cols + [f\"cuisine__{c}\" for c in top_k_cuisines]  # these are already numeric; we can treat them as 'passthrough' later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9fcb2",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Regression â€” predict numeric rating\n",
    "### 6.1 Linear Regression (scikitâ€‘learn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "if COL_RATING_NUM:\n",
    "    # Features: numeric + (Categorical OneHot)\n",
    "    X = df[num_cols_extended + cat_cols].copy() if cat_cols else df[num_cols_extended].copy()\n",
    "    y = df[COL_RATING_NUM]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    # Preprocess: impute and one-hot for categoricals\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]) if cat_cols else \"drop\"\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols_extended),\n",
    "            (\"cat\", categorical_transformer, cat_cols) if cat_cols else (\"cat\", \"drop\", []),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "                            (\"reg\", LinearRegression())])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    print(\"LinearRegression MSE:\", round(mse, 4))\n",
    "else:\n",
    "    print(\"No numeric rating column detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14a47a",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2 Linear Regression via Gradient Descent (from scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e748b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if COL_RATING_NUM:\n",
    "    # We'll use only numeric columns to keep implementation simple\n",
    "    Xg = df[num_cols_extended].copy()\n",
    "    yg = df[COL_RATING_NUM].copy()\n",
    "\n",
    "    # Impute missing numeric with median\n",
    "    for c in Xg.columns:\n",
    "        Xg[c] = Xg[c].fillna(Xg[c].median())\n",
    "    yg = yg.fillna(yg.median())\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(Xg.values)\n",
    "    # Add bias column\n",
    "    Xs = np.hstack([np.ones((Xs.shape[0], 1)), Xs])\n",
    "\n",
    "    # Train/test split\n",
    "    idx = np.arange(Xs.shape[0])\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    rng.shuffle(idx)\n",
    "    split = int(0.8 * len(idx))\n",
    "    tr_idx, te_idx = idx[:split], idx[split:]\n",
    "    Xtr, Xte = Xs[tr_idx], Xs[te_idx]\n",
    "    ytr, yte = yg.values[tr_idx], yg.values[te_idx]\n",
    "\n",
    "    # Gradient descent\n",
    "    w = np.zeros(Xtr.shape[1])\n",
    "    lr = 0.05\n",
    "    epochs = 2000\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        yhat = Xtr @ w\n",
    "        err = yhat - ytr\n",
    "        grad = (2.0 / len(Xtr)) * (Xtr.T @ err)\n",
    "        w -= lr * grad\n",
    "        if ep % 400 == 0:\n",
    "            tr_mse = np.mean((Xtr @ w - ytr)**2)\n",
    "            # print(f\"Epoch {ep}, train MSE {tr_mse:.4f}\")\n",
    "\n",
    "    mse_gd = np.mean((Xte @ w - yte)**2)\n",
    "    print(\"GradientDescent LinearReg MSE:\", round(mse_gd, 4))\n",
    "else:\n",
    "    print(\"No numeric rating column detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719c543",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Classification â€” Good/VeryGood/Excellent (1) **vs** Poor/Average (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "if y_cls is not None:\n",
    "    data_cls = df.copy()\n",
    "    data_cls[\"y\"] = y_cls\n",
    "    data_cls = data_cls.dropna(subset=[\"y\"])\n",
    "\n",
    "    Xc = data_cls[num_cols_extended + cat_cols].copy() if cat_cols else data_cls[num_cols_extended].copy()\n",
    "    yc = data_cls[\"y\"].astype(int)\n",
    "\n",
    "    Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, random_state=SEED, stratify=yc)\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "    categorical_transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                                              (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]) if cat_cols else \"drop\"\n",
    "\n",
    "    preprocessor_c = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols_extended),\n",
    "            (\"cat\", categorical_transformer, cat_cols) if cat_cols else (\"cat\", \"drop\", []),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    clf_lr = Pipeline(steps=[(\"preprocessor\", preprocessor_c),\n",
    "                             (\"clf\", LogisticRegression(max_iter=200))])\n",
    "\n",
    "    clf_lr.fit(Xc_train, yc_train)\n",
    "    pr = clf_lr.predict(Xc_test)\n",
    "    print(\"Logistic Regression metrics:\")\n",
    "    print(classification_report(yc_test, pr, digits=3))\n",
    "else:\n",
    "    print(\"No rating_text-derived binary target available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d307d77",
   "metadata": {},
   "source": [
    "\n",
    "### 7.1 Try three more classifiers and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca01a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def fit_eval(model, name):\n",
    "    pipe = Pipeline(steps=[(\"pre\", preprocessor_c), (\"clf\", model)])\n",
    "    pipe.fit(Xc_train, yc_train)\n",
    "    pred = pipe.predict(Xc_test)\n",
    "    p = precision_score(yc_test, pred, zero_division=0)\n",
    "    r = recall_score(yc_test, pred, zero_division=0)\n",
    "    f = f1_score(yc_test, pred, zero_division=0)\n",
    "    return {\"model\": name, \"precision\": p, \"recall\": r, \"f1\": f}\n",
    "\n",
    "if y_cls is not None:\n",
    "    results = []\n",
    "    results.append(fit_eval(RandomForestClassifier(n_estimators=200, random_state=SEED), \"RandomForest\"))\n",
    "    results.append(fit_eval(GradientBoostingClassifier(random_state=SEED), \"GradientBoosting\"))\n",
    "    results.append(fit_eval(SVC(kernel=\"rbf\", probability=False, random_state=SEED), \"SVC\"))\n",
    "\n",
    "    pd.DataFrame(results).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "else:\n",
    "    print(\"No rating_text-derived binary target available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af980f",
   "metadata": {},
   "source": [
    "\n",
    "## 8) PySpark equivalents (one regression, one classification)\n",
    "\n",
    "> If `pyspark` isn't installed in your environment, run `pip install pyspark` first (or install via conda).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eea1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed:\n",
    "# %pip install -q pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"eating_out_spark\").getOrCreate()\n",
    "sdf = spark.read.csv(DATA_CSV_PATH, header=True, inferSchema=True)\n",
    "sdf.printSchema()\n",
    "sdf.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c80c8a",
   "metadata": {},
   "source": [
    "\n",
    "### 8.1 PySpark â€” Regression (LinearRegression on numeric features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b28564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline as SparkPipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select a simple numeric-only feature set for demonstration\n",
    "reg_num_cols = [c for c in [COL_COST, COL_VOTES] if c]  # avoid very large OHE\n",
    "sdf_reg = sdf.select(*[c for c in reg_num_cols + [COL_RATING_NUM] if c is not None]).na.drop()\n",
    "\n",
    "va = VectorAssembler(inputCols=reg_num_cols, outputCol=\"features\")\n",
    "reg = LinearRegression(featuresCol=\"features\", labelCol=COL_RATING_NUM)\n",
    "sp = SparkPipeline(stages=[va, reg])\n",
    "\n",
    "spr = sp.fit(sdf_reg)\n",
    "pred = spr.transform(sdf_reg)\n",
    "RegressionEvaluator(labelCol=COL_RATING_NUM, predictionCol=\"prediction\", metricName=\"rmse\").evaluate(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127394d5",
   "metadata": {},
   "source": [
    "\n",
    "### 8.2 PySpark â€” Classification (LogisticRegression with simple features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4796147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Build binary label\n",
    "if COL_RATING_TEXT:\n",
    "    sdf_cls = sdf.withColumn(\n",
    "        \"label\",\n",
    "        F.when(F.col(COL_RATING_TEXT).isin(\"Poor\", \"Average\"), F.lit(0))\n",
    "         .when(F.col(COL_RATING_TEXT).isin(\"Good\", \"Very Good\", \"Excellent\"), F.lit(1))\n",
    "         .otherwise(F.lit(None).cast(\"int\"))\n",
    "    ).na.drop(subset=[\"label\"])\n",
    "\n",
    "    # Choose a compact numeric feature set for demo\n",
    "    cls_num_cols = [c for c in [COL_COST, COL_VOTES] if c]\n",
    "    sdf_cls = sdf_cls.select(*[c for c in cls_num_cols + [\"label\"] if c is not None]).na.drop()\n",
    "\n",
    "    va = VectorAssembler(inputCols=cls_num_cols, outputCol=\"features\")\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "    spc = SparkPipeline(stages=[va, lr])\n",
    "\n",
    "    m = spc.fit(sdf_cls)\n",
    "    pred = m.transform(sdf_cls)\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    evaluator.evaluate(pred)\n",
    "else:\n",
    "    print(\"COL_RATING_TEXT not found; cannot build binary label in Spark.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fe2ec",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Reproducibility scaffolding (Git + Git LFS + DVC)\n",
    "\n",
    "> Run these in a terminal (or a notebook cell with `!` prefix) **in your project folder**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2309746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shell commands are commented so they don't run accidentally in some environments.\n",
    "# Remove the leading '#' to execute in a local environment.\n",
    "\n",
    "# Initialize Git & Git LFS\n",
    "# !git init\n",
    "# !git lfs install\n",
    "# !git lfs track \"*.csv\" \"*.geojson\" \"*.ipynb\" \"models/*.pkl\"\n",
    "# !git add .gitattributes\n",
    "\n",
    "# Initialize DVC & track data\n",
    "# !dvc init\n",
    "# !dvc add zomato_df_final_data.csv\n",
    "# !dvc add sydney.geojson\n",
    "# !git add zomato_df_final_data.csv.dvc sydney.geojson.dvc .dvc .gitignore\n",
    "# !git commit -m \"Init project with data tracked by DVC\"\n",
    "\n",
    "# Example dvc.yaml stages (write this out to dvc.yaml):\n",
    "dvc_yaml = \\\"\\\"\\\"\n",
    "stages:\n",
    "  preprocess:\n",
    "    cmd: python scripts/preprocess.py\n",
    "    deps:\n",
    "    - scripts/preprocess.py\n",
    "    - zomato_df_final_data.csv\n",
    "    outs:\n",
    "    - data/clean.parquet\n",
    "  features:\n",
    "    cmd: python scripts/features.py\n",
    "    deps:\n",
    "    - scripts/features.py\n",
    "    - data/clean.parquet\n",
    "    outs:\n",
    "    - data/features.parquet\n",
    "  train_reg:\n",
    "    cmd: python scripts/train_reg.py\n",
    "    deps:\n",
    "    - scripts/train_reg.py\n",
    "    - data/features.parquet\n",
    "    outs:\n",
    "    - models/reg.pkl\n",
    "    metrics:\n",
    "    - metrics/reg.json\n",
    "  train_cls:\n",
    "    cmd: python scripts/train_cls.py\n",
    "    deps:\n",
    "    - scripts/train_cls.py\n",
    "    - data/features.parquet\n",
    "    outs:\n",
    "    - models/cls.pkl\n",
    "    metrics:\n",
    "    - metrics/cls.json\n",
    "\\\"\\\"\\\"\n",
    "print(dvc_yaml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46aa65",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- Some column names may differ across dataset versions. The code tries to **auto-detect** common names and will print what it found at runtime.\n",
    "- For geospatial choropleths, inspect which property in the GeoJSON represents the suburb name (the code guesses, but you can override by renaming the GeoDataFrame column to `suburb_key` for the merge).\n",
    "\n",
    "Good luck! ðŸŽ¯\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}